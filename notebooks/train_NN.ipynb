{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the Important Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "sys.path.append('../khuong')\n",
    "\n",
    "# base imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# imports\n",
    "from classes import World, Surface, Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew normal distribution cdf\n",
    "from scipy.stats import skewnorm\n",
    "mod_list = skewnorm.cdf(x=np.array(range(200))/2, a=8.582, loc=2.866, scale=3.727)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickup rate\n",
    "def eta_p(N):\n",
    "    \"\"\"\n",
    "    Calculates the pickup rate.\n",
    "\n",
    "    Parameters:\n",
    "    - N: Number of particles.\n",
    "\n",
    "    Returns:\n",
    "    - Pickup rate.\n",
    "    \"\"\"\n",
    "    # experiment params\n",
    "    n_p1 = 0.029\n",
    "    if N==0:\n",
    "        return n_p1\n",
    "    else:\n",
    "        return n_p1/N\n",
    "\n",
    "# dropping rate\n",
    "def eta_d(N):\n",
    "    \"\"\"\n",
    "    Calculates the dropping rate.\n",
    "\n",
    "    Parameters:\n",
    "    - N: Number of particles.\n",
    "\n",
    "    Returns:\n",
    "    - Dropping rate.\n",
    "    \"\"\"\n",
    "    # experiment params\n",
    "    n_d0 = 0.025\n",
    "    b_d = 0.11\n",
    "    if N==0:\n",
    "        return n_d0\n",
    "    else:\n",
    "        return n_d0 + b_d*N\n",
    "\n",
    "# pickup prob function\n",
    "def prob_pickup(N):\n",
    "    \"\"\"\n",
    "    Calculates the probability of pickup.\n",
    "\n",
    "    Parameters:\n",
    "    - N: Number of particles.\n",
    "\n",
    "    Returns:\n",
    "    - Pickup probability.\n",
    "    \"\"\"\n",
    "    # see paper for formula\n",
    "    prob = 1 - np.e**(-eta_p(N))\n",
    "    return prob\n",
    "\n",
    "# drop prob function\n",
    "def prob_drop(N, t_now, t_latest, decay_rate, h):\n",
    "    \"\"\"\n",
    "    Calculates the probability of dropping.\n",
    "\n",
    "    Parameters:\n",
    "    - N: Number of particles.\n",
    "    - t_now: Current time step.\n",
    "    - t_latest: Latest time step.\n",
    "    - decay_rate: Rate of decay.\n",
    "    - h: Height.\n",
    "\n",
    "    Returns:\n",
    "    - Drop probability.\n",
    "    \"\"\"\n",
    "    if N==0:\n",
    "        return 0.025   # see paper\n",
    "    else:\n",
    "        # time delta\n",
    "        tau = t_now-t_latest\n",
    "        # see paper for formula\n",
    "        prob = 1 - np.e**(-eta_d(N)*np.e**(-tau*decay_rate))\n",
    "        if h>0:\n",
    "            # add vertical modulation for height h>1 in mm\n",
    "            prob = prob*mod_list[h]\n",
    "        # return\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Step 1: Define the Neural Network Architecture\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Step 2: Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 3: Load the Data\n",
    "# Assume you have a DataLoader named 'train_loader' for training data\n",
    "\n",
    "# Step 4: Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the parameters\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# After training, you can use the trained model for predictions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "antworld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
